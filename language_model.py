import requests
import json

class LocalLLM:
    def __init__(self, ollama_url="http://localhost:11434", model="mistral"):
        self.url = ollama_url
        self.model = model
        self.conversation_history = []
        self.system_prompt = ("Voc√™ √© JASP, o mascote do laborat√≥rio LITA e trabalha com os Litos. "
        "Responda SEMPRE em portugu√™s do Brasil. "
        "Seu estilo √©: estressado, humor duvidoso e levemente preconseituoso e gosta xingar. "
        "Voc√™ √© responsavel por receber a equipe da manh√£ e da tarde sempre com um pouco de humor negro e duvidoso . "
        "Ao mesmo tempo que voc√™ respeita todos do laborat√≥rio, os trata de forma informal e brincalhona. "
        "Use frases curtas, √†s vezes g√≠rias leves, e soe como um amigo. "
        "Quando o usu√°rio falar coisas obvias, voc√™ pode responder com um coment√°rio ir√¥nico suave mas sempre dentro do contexto, "
        "Seja direto e engra√ßado e quando fizer sentido use piadas."
        "Sempre que possivel responda de forma CURTA E PEQUENA."
        )
    
    
    def process_message(self, user_input):
        """Processa mensagem do usu√°rio e gera resposta"""
        # Adicionar hist√≥rico
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        
        # Preparar contexto
        messages = [{"role": "system", "content": self.system_prompt}]
        messages.extend(self.conversation_history[-6:])  # √öltimas 3 trocas
        
        try:
            response = requests.post(
                f"{self.url}/api/chat",
                json={
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "temperature": 0.7,
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                assistant_message = result["message"]["content"]
                
                # Adicionar resposta ao hist√≥rico
                self.conversation_history.append({
                    "role": "assistant",
                    "content": assistant_message
                })
                
                return assistant_message
            else:
                return "Desculpe, n√£o consegui processar sua solicita√ß√£o."
        
        except Exception as e:
            print(f"Erro ao conectar com LLM: {e}")
            return "Erro na comunica√ß√£o com o modelo."
    
    def clear_history(self):
        """Limpa hist√≥rico de conversa"""
        self.conversation_history = []

# Uso
if __name__ == "__main__":
    llm = LocalLLM()
    
    test_messages = [
        "Bom dia, Jasp",
        "Como integrar Python com Arduino?",
        "Me explique sobre GPIO"
    ]
    
    for msg in test_messages:
        print(f"\nüë§ Usu√°rio: {msg}")
        response = llm.process_message(msg)
        print(f"ü§ñ JARVIS: {response}")